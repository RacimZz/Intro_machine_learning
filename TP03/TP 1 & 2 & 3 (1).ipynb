{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f80e190",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.2       , 0.62352941, 0.99215686, 0.62352941, 0.19607843,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.18823529,\n",
       "        0.93333333, 0.98823529, 0.98823529, 0.98823529, 0.92941176,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.21176471, 0.89019608,\n",
       "        0.99215686, 0.98823529, 0.9372549 , 0.91372549, 0.98823529,\n",
       "        0.22352941, 0.02352941, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.03921569, 0.23529412, 0.87843137, 0.98823529,\n",
       "        0.99215686, 0.98823529, 0.79215686, 0.32941176, 0.98823529,\n",
       "        0.99215686, 0.47843137, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.63921569, 0.98823529, 0.98823529, 0.98823529,\n",
       "        0.99215686, 0.98823529, 0.98823529, 0.37647059, 0.74117647,\n",
       "        0.99215686, 0.65490196, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.2       , 0.93333333, 0.99215686, 0.99215686, 0.74509804,\n",
       "        0.44705882, 0.99215686, 0.89411765, 0.18431373, 0.30980392,\n",
       "        1.        , 0.65882353, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.18823529,\n",
       "        0.93333333, 0.98823529, 0.98823529, 0.70196078, 0.04705882,\n",
       "        0.29411765, 0.4745098 , 0.08235294, 0.        , 0.        ,\n",
       "        0.99215686, 0.95294118, 0.19607843, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.14901961, 0.64705882,\n",
       "        0.99215686, 0.91372549, 0.81568627, 0.32941176, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.99215686, 0.98823529, 0.64705882, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.02745098, 0.69803922, 0.98823529,\n",
       "        0.94117647, 0.27843137, 0.0745098 , 0.10980392, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.99215686, 0.98823529, 0.76470588, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.22352941, 0.98823529, 0.98823529,\n",
       "        0.24705882, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.99215686, 0.98823529, 0.76470588, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.77647059, 0.99215686, 0.74509804,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        1.        , 0.99215686, 0.76862745, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.29803922, 0.96470588, 0.98823529, 0.43921569,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.99215686, 0.98823529, 0.58039216, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.33333333, 0.98823529, 0.90196078, 0.09803922,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.02745098, 0.52941176,\n",
       "        0.99215686, 0.72941176, 0.04705882, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.33333333, 0.98823529, 0.8745098 , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.02745098, 0.51372549, 0.98823529,\n",
       "        0.88235294, 0.27843137, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.33333333, 0.98823529, 0.56862745, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.18823529, 0.64705882, 0.98823529, 0.67843137,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.3372549 , 0.99215686, 0.88235294, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.44705882, 0.93333333, 0.99215686, 0.63529412, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.33333333, 0.98823529, 0.97647059, 0.57254902,\n",
       "        0.18823529, 0.11372549, 0.33333333, 0.69803922, 0.88235294,\n",
       "        0.99215686, 0.8745098 , 0.65490196, 0.21960784, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.33333333, 0.98823529, 0.98823529, 0.98823529,\n",
       "        0.89803922, 0.84313725, 0.98823529, 0.98823529, 0.98823529,\n",
       "        0.76862745, 0.50980392, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.10980392, 0.78039216, 0.98823529, 0.98823529,\n",
       "        0.99215686, 0.98823529, 0.98823529, 0.91372549, 0.56862745,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.09803922, 0.50196078, 0.98823529,\n",
       "        0.99215686, 0.98823529, 0.55294118, 0.14509804, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Placement de ma base de donneés\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "X , y = fetch_openml(name = 'mnist_784' , version = 1 , return_X_y=True )\n",
    "X = X.to_numpy()\n",
    "X = X/255\n",
    "y = y.to_numpy()\n",
    "X[1].reshape((28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6551543",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13965, 784)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "X3 = X[(y== '3')]\n",
    "X4 = X[(y== '4')]\n",
    "\n",
    "X34 = X[(y == '3') | (y =='4')]\n",
    "y34 = y[(y =='3') | (y =='4')] \n",
    "X34.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec198e5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "#affichage des chiffres 3 et 4\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import svm\n",
    "import numpy as np\n",
    "from skimage.feature import hog\n",
    "\n",
    "fig , (ax1 , ax2)= plt.subplots (1,2 ,figsize =(16 , 4))\n",
    "\n",
    "ax1.imshow(X3[0].reshape((28,28)))\n",
    "ax1.set_title('image1')\n",
    "\n",
    "ax2.imshow(X4[0].reshape((28,28)))\n",
    "ax2.set_title('image2')\n",
    "\n",
    "plt.show() \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c74789f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade scikit-image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd1cc895",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "import numpy as np\n",
    "\n",
    "#classification binaire linéaire\n",
    "X_train, X_test, y_train, y_test = train_test_split(X34, y34, test_size=0.5)\n",
    "\n",
    "\n",
    "classifier = svm.SVC(kernel='linear')\n",
    "classifier.fit(X_train , y_train)\n",
    "\n",
    "y_est = classifier.predict( X_train)\n",
    "classifier.score( X_train , y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c363e4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classification générique (rbf)\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import svm\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X34, y34, test_size=0.8)\n",
    "\n",
    "\n",
    "classifier_rbf = svm.SVC(kernel='rbf')\n",
    "classifier_rbf.fit(X_train , y_train)\n",
    "\n",
    "y_est = classifier.predict( X_train)\n",
    "classifier.score( X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05deb904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM linéaire :\n",
      "test :  0.88\n",
      "Train :  1.0\n",
      "\n",
      "SVM non linéaire (rbf) :\n",
      "Test :  0.89\n",
      "Train :  0.9825\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from sklearn import svm\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "#comparatif des scores de classification rbf & linear\n",
    "\n",
    "X_s = X[:5000]\n",
    "y_s= y[:5000]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_s, y_s, test_size=0.2)\n",
    "\n",
    "classifier = svm.SVC(kernel='linear')\n",
    "classifier.fit(X_train , y_train)\n",
    "\n",
    "print(\"SVM linéaire :\")\n",
    "y_est = classifier.predict( X_test)\n",
    "print(\"test : \",classifier.score( X_test, y_test))\n",
    "\n",
    "y_est = classifier.predict( X_train)\n",
    "print(\"Train : \",classifier.score( X_train, y_train))\n",
    "\n",
    "classifier_rbf = svm.SVC(kernel='rbf')\n",
    "classifier_rbf.fit(X_train , y_train)\n",
    "\n",
    "print(\"\\nSVM non linéaire (rbf) :\")\n",
    "y_est = classifier_rbf.predict( X_test)\n",
    "print(\"Test : \",classifier_rbf.score( X_test, y_test))\n",
    "\n",
    "y_est = classifier_rbf.predict( X_train)\n",
    "print(\"Train : \",classifier_rbf.score( X_train, y_train))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a33aca6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAAAMA0lEQVR4nO3da4xcdR3G8eexLtWUEluR2kAjQuoFjBbdVCJquESEvrAYo1KNqQlmfQERE00kaiIviReIJoRkkUo1iEGRUBOi1mokvqDpQmrpRQpixdK1q6kKmliW9ueLPehSZs5u59zG/X0/yWRmzv/MnCeTffbMzDm7f0eEACx8L+s6AIB2UHYgCcoOJEHZgSQoO5DEy9vc2CleHK/QkjY3CaTyb/1Lz8VR9xqrVHbbV0j6pqRFkr4dETeVrf8KLdE7fVmVTQIosT229R0b+G287UWSbpV0paTzJG2wfd6gzwegWVU+s6+V9EREPBkRz0n6gaT19cQCULcqZT9T0p9m3T9YLHsR22O2J2xPTOtohc0BqKJK2Xt9CfCSc28jYjwiRiNidESLK2wOQBVVyn5Q0qpZ98+SdKhaHABNqVL2HZJW23697VMkXS1pSz2xANRt4ENvEfG87esk/Uwzh942RcSe2pIBqFWl4+wR8YCkB2rKAqBBnC4LJEHZgSQoO5AEZQeSoOxAEpQdSIKyA0lQdiAJyg4kQdmBJCg7kARlB5Kg7EASlB1IgrIDSVB2IAnKDiRB2YEkKDuQBGUHkqDsQBKtTtmM9u3/zjtKx//w/jtKx28+ck7p+C8+Mlo6fmzv/tJxtIc9O5AEZQeSoOxAEpQdSIKyA0lQdiAJyg4kwXH2BWDR+W/sO3b/JbeWPnY6RkrHr132WOn4j956een40r2lw2hRpbLbPiDpWUnHJD0fEeVnWADoTB179ksi4q81PA+ABvGZHUiiatlD0s9tP2x7rNcKtsdsT9iemNbRipsDMKiqb+MviohDts+QtNX27yLiwdkrRMS4pHFJOs3Lo+L2AAyo0p49Ig4V11OS7pO0to5QAOo3cNltL7G99IXbki6XtLuuYADqVeVt/ApJ99l+4Xm+HxE/rSUVTs7Tf+479Jn9V5c+dOv599adBkNq4LJHxJOS3lZjFgAN4tAbkARlB5Kg7EASlB1IgrIDSfAnrgvAsb//o+/YHw+uLn/w+TWHwdBizw4kQdmBJCg7kARlB5Kg7EASlB1IgrIDSXCcfQFYtOKMvmPveTNTJmMGe3YgCcoOJEHZgSQoO5AEZQeSoOxAEpQdSILj7AvB0iV9h9Yt39Hopqfe4dLxV+16Q9+xY3s5B6BN7NmBJCg7kARlB5Kg7EASlB1IgrIDSVB2IAmOsy8Ax574Q9+xL//ko6WP/dCGWytte8/HvlU6fsE/ru87torj7K2ac89ue5PtKdu7Zy1bbnur7ceL62XNxgRQ1Xzext8p6YoTlt0gaVtErJa0rbgPYIjNWfaIeFDSkRMWr5e0ubi9WdJV9cYCULdBv6BbERGTklRc9/0naLbHbE/YnpjW0QE3B6Cqxr+Nj4jxiBiNiNERLW56cwD6GLTsh22vlKTieqq+SACaMGjZt0jaWNzeKOn+euIAaMqcx9lt3y3pYkmn2z4o6SuSbpJ0j+1rJD0l6cNNhsTgzv38Q+UrbGgnB7o3Z9kjot+Pw2U1ZwHQIE6XBZKg7EASlB1IgrIDSVB2IAn+xDW5ES8qHZ+OloKgcezZgSQoO5AEZQeSoOxAEpQdSIKyA0lQdiAJjrMnNx3HSseP63hLSdA09uxAEpQdSIKyA0lQdiAJyg4kQdmBJCg7kARlB5Kg7EASlB1IgrIDSVB2IAnKDiRB2YEkKDuQBGUHkpiz7LY32Z6yvXvWshttP217Z3FZ12xMAFXNZ89+p6Qreiy/JSLWFJcH6o0FoG5zlj0iHpR0pIUsABpU5TP7dbZ3FW/zl/VbyfaY7QnbE9M6WmFzAKoYtOy3STpX0hpJk5K+0W/FiBiPiNGIGB3R4gE3B6CqgcoeEYcj4lhEHJd0u6S19cYCULeBym575ay7H5S0u9+6AIbDnP833vbdki6WdLrtg5K+Iuli22skhaQDkj7dXEQ0qen52U9711S1J0Bt5ix7RGzosfiOBrIAaBBn0AFJUHYgCcoOJEHZgSQoO5AEUzYn1/SUzb9+2919xz5w4TXlD35oV6Vt48XYswNJUHYgCcoOJEHZgSQoO5AEZQeSoOxAEhxnT+5Nv/xU6fjeS8cb2/b+sVNKx9/wUGObTok9O5AEZQeSoOxAEpQdSIKyA0lQdiAJyg4kwXH25Bbvf2X5Cpe2kwPNY88OJEHZgSQoO5AEZQeSoOxAEpQdSIKyA0k4ouKcvCfhNC+Pd/qy1raH6jb87lDp+MeXTg783HNNF33llb0mEP6f47/dN/C2F6rtsU3PxBH3Gptzz257le1f2d5ne4/t64vly21vtf14cb2s7uAA6jOft/HPS/pcRLxZ0oWSrrV9nqQbJG2LiNWSthX3AQypOcseEZMR8Uhx+1lJ+ySdKWm9pM3FapslXdVQRgA1OKkv6GyfLekCSdslrYiISWnmF4KkM/o8Zsz2hO2JaR2tGBfAoOZddtunSrpX0mcj4pn5Pi4ixiNiNCJGR7R4kIwAajCvstse0UzR74qIHxeLD9teWYyvlDTVTEQAdZjzT1xtW9IdkvZFxM2zhrZI2ijppuL6/kYSolN3PvWu0vEN5/9w4Oeebu+oLzS/v2e/SNInJD1qe2ex7IuaKfk9tq+R9JSkDzeSEEAt5ix7RPxGUs+D9JI4Qwb4P8HpskASlB1IgrIDSVB2IAnKDiTBv5JGqaN3vrZ8ha+1kwPVsWcHkqDsQBKUHUiCsgNJUHYgCcoOJEHZgSQ4zo5Sy3YeKR2/9W9vLB2/dtljdcZBBezZgSQoO5AEZQeSoOxAEpQdSIKyA0lQdiAJpmwGFpBKUzYDWBgoO5AEZQeSoOxAEpQdSIKyA0lQdiCJOctue5XtX9neZ3uP7euL5Tfaftr2zuKyrvm4AAY1n39e8bykz0XEI7aXSnrY9tZi7JaI+Hpz8QDUZT7zs09KmixuP2t7n6Qzmw4GoF4n9Znd9tmSLpC0vVh0ne1dtjfZXtbnMWO2J2xPTOtotbQABjbvsts+VdK9kj4bEc9Iuk3SuZLWaGbP/41ej4uI8YgYjYjRES2unhjAQOZVdtsjmin6XRHxY0mKiMMRcSwijku6XdLa5mICqGo+38Zb0h2S9kXEzbOWr5y12gcl7a4/HoC6zOfb+IskfULSo7Z3Fsu+KGmD7TWSQtIBSZ9uIB+Amszn2/jfSOr197EP1B8HQFM4gw5IgrIDSVB2IAnKDiRB2YEkKDuQBGUHkqDsQBKUHUiCsgNJUHYgCcoOJEHZgSQoO5BEq1M22/6LpD/OWnS6pL+2FuDkDGu2Yc0lkW1QdWZ7XUS8ptdAq2V/ycbtiYgY7SxAiWHNNqy5JLINqq1svI0HkqDsQBJdl3284+2XGdZsw5pLItugWsnW6Wd2AO3pes8OoCWUHUiik7LbvsL2Y7afsH1DFxn6sX3A9qPFNNQTHWfZZHvK9u5Zy5bb3mr78eK65xx7HWUbimm8S6YZ7/S163r689Y/s9teJGm/pPdJOihph6QNEbG31SB92D4gaTQiOj8Bw/Z7Jf1T0ncj4i3Fsq9KOhIRNxW/KJdFxBeGJNuNkv7Z9TTexWxFK2dPMy7pKkmfVIevXUmuj6iF162LPftaSU9ExJMR8ZykH0ha30GOoRcRD0o6csLi9ZI2F7c3a+aHpXV9sg2FiJiMiEeK289KemGa8U5fu5Jcreii7GdK+tOs+wc1XPO9h6Sf237Y9ljXYXpYERGT0swPj6QzOs5zojmn8W7TCdOMD81rN8j051V1UfZeU0kN0/G/iyLi7ZKulHRt8XYV8zOvabzb0mOa8aEw6PTnVXVR9oOSVs26f5akQx3k6CkiDhXXU5Lu0/BNRX34hRl0i+upjvP81zBN491rmnENwWvX5fTnXZR9h6TVtl9v+xRJV0va0kGOl7C9pPjiRLaXSLpcwzcV9RZJG4vbGyXd32GWFxmWabz7TTOujl+7zqc/j4jWL5LWaeYb+d9L+lIXGfrkOkfSb4vLnq6zSbpbM2/rpjXzjugaSa+WtE3S48X18iHK9j1Jj0rapZlirewo27s189Fwl6SdxWVd169dSa5WXjdOlwWS4Aw6IAnKDiRB2YEkKDuQBGUHkqDsQBKUHUjiP5tIq1puS9GRAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1152x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#   Classification des representations HOG\n",
    "\n",
    "fig , ax1= plt.subplots (1 ,figsize =(16 , 4))\n",
    "vec = X[6].reshape((28 ,28))\n",
    "ax1.imshow(vec)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8721811",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'skimage'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_53/4011040228.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mskimage\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhog\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msvm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'skimage'"
     ]
    }
   ],
   "source": [
    "from skimage . feature import hog\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import svm\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#2.2\n",
    "\n",
    "fig , ax1= plt.subplots (1 ,figsize =(16 , 4))\n",
    "\n",
    "vec = X34[7].reshape(28 ,28)\n",
    "ax1.imshow(vec)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "X34 , y34 = X34[:1000] , y34[:1000]\n",
    "\n",
    "features = []\n",
    "\n",
    "for i in range(1000):\n",
    "    features.append(hog(X34[i].reshape((28, 28)), pixels_per_cell=(8, 8), cells_per_block=(2, 2)))\n",
    "    #hog extrait les caracteristique de chaque image X34[i] et on les stocke dans un tableau\n",
    "X_hog = np.array(features)\n",
    "plt.hist(X_hog[0])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_hog,y , test_size=0.8)\n",
    "\n",
    "classifier_rbf = svm.SVC(kernel='rbf')\n",
    "classifier_rbf.fit(X_train , y_train)\n",
    "print(\"SVM RBF :\")\n",
    "y_est = classifier_rbf.predict( X_test)\n",
    "print(\"Test: \",classifier_rbf.score( X_test, y_test))\n",
    "\n",
    "y_est = classifier_rbf.predict( X_train)\n",
    "print(\"Train: \",classifier_rbf.score( X_train, y_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d21510f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.2       , 0.62352941, 0.99215686, 0.62352941, 0.19607843,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.18823529,\n",
       "        0.93333333, 0.98823529, 0.98823529, 0.98823529, 0.92941176,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.21176471, 0.89019608,\n",
       "        0.99215686, 0.98823529, 0.9372549 , 0.91372549, 0.98823529,\n",
       "        0.22352941, 0.02352941, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.03921569, 0.23529412, 0.87843137, 0.98823529,\n",
       "        0.99215686, 0.98823529, 0.79215686, 0.32941176, 0.98823529,\n",
       "        0.99215686, 0.47843137, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.63921569, 0.98823529, 0.98823529, 0.98823529,\n",
       "        0.99215686, 0.98823529, 0.98823529, 0.37647059, 0.74117647,\n",
       "        0.99215686, 0.65490196, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.2       , 0.93333333, 0.99215686, 0.99215686, 0.74509804,\n",
       "        0.44705882, 0.99215686, 0.89411765, 0.18431373, 0.30980392,\n",
       "        1.        , 0.65882353, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.18823529,\n",
       "        0.93333333, 0.98823529, 0.98823529, 0.70196078, 0.04705882,\n",
       "        0.29411765, 0.4745098 , 0.08235294, 0.        , 0.        ,\n",
       "        0.99215686, 0.95294118, 0.19607843, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.14901961, 0.64705882,\n",
       "        0.99215686, 0.91372549, 0.81568627, 0.32941176, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.99215686, 0.98823529, 0.64705882, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.02745098, 0.69803922, 0.98823529,\n",
       "        0.94117647, 0.27843137, 0.0745098 , 0.10980392, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.99215686, 0.98823529, 0.76470588, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.22352941, 0.98823529, 0.98823529,\n",
       "        0.24705882, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.99215686, 0.98823529, 0.76470588, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.77647059, 0.99215686, 0.74509804,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        1.        , 0.99215686, 0.76862745, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.29803922, 0.96470588, 0.98823529, 0.43921569,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.99215686, 0.98823529, 0.58039216, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.33333333, 0.98823529, 0.90196078, 0.09803922,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.02745098, 0.52941176,\n",
       "        0.99215686, 0.72941176, 0.04705882, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.33333333, 0.98823529, 0.8745098 , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.02745098, 0.51372549, 0.98823529,\n",
       "        0.88235294, 0.27843137, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.33333333, 0.98823529, 0.56862745, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.18823529, 0.64705882, 0.98823529, 0.67843137,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.3372549 , 0.99215686, 0.88235294, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.44705882, 0.93333333, 0.99215686, 0.63529412, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.33333333, 0.98823529, 0.97647059, 0.57254902,\n",
       "        0.18823529, 0.11372549, 0.33333333, 0.69803922, 0.88235294,\n",
       "        0.99215686, 0.8745098 , 0.65490196, 0.21960784, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.33333333, 0.98823529, 0.98823529, 0.98823529,\n",
       "        0.89803922, 0.84313725, 0.98823529, 0.98823529, 0.98823529,\n",
       "        0.76862745, 0.50980392, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.10980392, 0.78039216, 0.98823529, 0.98823529,\n",
       "        0.99215686, 0.98823529, 0.98823529, 0.91372549, 0.56862745,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.09803922, 0.50196078, 0.98823529,\n",
       "        0.99215686, 0.98823529, 0.55294118, 0.14509804, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Rechargeemnt de ma base de donneés\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "X , y = fetch_openml(name = 'mnist_784' , version = 1 , return_X_y=True )\n",
    "X = X.to_numpy()\n",
    "X = X/255\n",
    "y = y.to_numpy()\n",
    "X[1].reshape((28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9df04a8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_143/1893504299.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mfig\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0max1\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mfigsize\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "#2.2\n",
    "from skimage . feature import hog\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import svm\n",
    "import numpy as np\n",
    "\n",
    "X , y = X[:1000] , y[:1000]\n",
    "\n",
    "fig , ax1= plt.subplots (1 ,figsize =(16 , 4))\n",
    "\n",
    "vec = X[55].reshape(28 ,28)\n",
    "ax1.imshow(vec)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "features = []\n",
    "\n",
    "for i in range(1000):\n",
    "    features.append(hog(X[i].reshape((28,28))))\n",
    "\n",
    "X_hog = np.array(features)\n",
    "plt.hist(X_hog[0])\n",
    "\n",
    " #2.2\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_hog,y , test_size=0.8)\n",
    "\n",
    "classifier = svm.SVC(kernel='linear',probability=True)\n",
    "classifier.fit(X_train , y_train)\n",
    "print(\"Classifieur SVM linéaire:\")\n",
    "y_est = classifier.predict( X_test)\n",
    "print(\"Test : \",classifier.score( X_test, y_test))\n",
    "\n",
    "y_est = classifier.predict( X_train)\n",
    "print(\"Train : \",classifier.score( X_train, y_train))\n",
    "\n",
    "classifier . predict_proba (X_hog)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "classifier_rbf = svm.SVC(kernel='rbf',probability=True)\n",
    "classifier_rbf.fit(X_train , y_train)\n",
    "print(\"Classifieur SVM RBF:\")\n",
    "\n",
    "y_est = classifier_rbf.predict( X_test)\n",
    "print(\"Test_rbf : \",classifier_rbf.score( X_test, y_test))\n",
    "\n",
    "y_est = classifier_rbf.predict( X_train)\n",
    "print(\"Train_rbf : \",classifier_rbf.score( X_train, y_train))\n",
    "\n",
    "classifier . predict_proba (X_hog)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a9683537",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nUn seul voisin (k=1) : Risque de surajustement (overfitting) car il est sensible au bruit dans les données,\\nmais peut être instable.\\n\\nTrop de voisins (k élevé) : Risque de sous-ajustement (underfitting) car il ignore les détails locaux des données.\\n\\nNombre impair de voisins (et même nombre premier) : Préférable car il évite les votes à égalité, \\nréduit le biais en faveur d'une classe, et permet une meilleure capture des frontières entre les classes.\\n\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Classification kNN\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Un seul voisin (k=1) : Risque de surajustement (overfitting) car il est sensible au bruit dans les données,\n",
    "mais peut être instable.\n",
    "\n",
    "Trop de voisins (k élevé) : Risque de sous-ajustement (underfitting) car il ignore les détails locaux des données.\n",
    "\n",
    "Nombre impair de voisins (et même nombre premier) : Préférable car il évite les votes à égalité, \n",
    "réduit le biais en faveur d'une classe, et permet une meilleure capture des frontières entre les classes.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d0669acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN =\n",
      " ['7' '9' '9' '3' '4' '6' '2' '5' '2' '4' '2' '5' '1' '4' '1' '8' '9' '5'\n",
      " '5' '1' '7' '0' '5' '7' '2' '1' '9' '2' '6' '5' '4' '2' '4' '1' '7' '6'\n",
      " '0' '7' '7' '1' '6' '2' '7' '2' '1' '3' '1' '2' '8' '8' '2' '0' '2' '1'\n",
      " '7' '3' '1' '9' '1' '9' '9' '1' '6' '2' '3' '9' '1' '0' '9' '2' '7' '9'\n",
      " '4' '6' '0' '0' '2' '9' '9' '1' '1' '4' '1' '7' '2' '7' '0' '2' '2' '1'\n",
      " '6' '0' '4' '5' '0' '8' '8' '3' '8' '7' '6' '6' '1' '5' '2' '3' '2' '0'\n",
      " '6' '7' '4' '7' '3' '6' '0' '0' '3' '3' '6' '0' '5' '9' '7' '1' '3' '9'\n",
      " '1' '1' '7' '7' '5' '4' '1' '2' '8' '8' '4' '5' '3' '2' '1' '4' '9' '6'\n",
      " '9' '7' '1' '9' '8' '1' '7' '9' '5' '7' '3' '0' '1' '2' '7' '5' '9' '3'\n",
      " '9' '0' '6' '1' '3' '9' '5' '8' '1' '3' '6' '1' '9' '8' '3' '9' '6' '4'\n",
      " '5' '5' '7' '5' '0' '3' '1' '1' '9' '6' '3' '1' '7' '5' '0' '9' '7' '1'\n",
      " '7' '4']\n",
      "SVM =\n",
      " ['7' '9' '9' '3' '4' '6' '2' '5' '2' '4' '2' '1' '1' '2' '1' '8' '9' '7'\n",
      " '5' '1' '7' '0' '5' '7' '2' '1' '9' '2' '5' '5' '4' '2' '4' '1' '7' '6'\n",
      " '0' '7' '7' '1' '6' '2' '7' '2' '4' '3' '1' '2' '8' '8' '2' '0' '2' '1'\n",
      " '7' '3' '1' '9' '8' '9' '9' '1' '6' '2' '3' '4' '1' '0' '9' '8' '7' '9'\n",
      " '4' '5' '0' '0' '8' '9' '9' '1' '1' '4' '1' '7' '2' '7' '0' '2' '2' '1'\n",
      " '6' '0' '4' '5' '0' '8' '8' '5' '8' '7' '6' '6' '1' '3' '2' '3' '2' '0'\n",
      " '5' '7' '4' '7' '3' '6' '0' '0' '3' '3' '6' '0' '5' '9' '7' '4' '3' '9'\n",
      " '1' '5' '7' '7' '5' '4' '1' '2' '8' '8' '4' '5' '3' '2' '1' '4' '9' '6'\n",
      " '9' '2' '1' '9' '1' '8' '7' '9' '5' '7' '3' '0' '5' '2' '7' '5' '9' '3'\n",
      " '9' '0' '6' '1' '3' '9' '3' '8' '7' '3' '6' '1' '7' '5' '3' '4' '6' '5'\n",
      " '5' '5' '7' '5' '0' '2' '1' '1' '4' '6' '3' '1' '7' '5' '0' '9' '7' '1'\n",
      " '7' '4']\n",
      "temps Knn :  0.1299574375152588 \n",
      "temps Svm :  0.5926613807678223\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "t_begin_knn = time.time()\n",
    "#kNN\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=1)\n",
    "knn_classifier.fit(X_train, y_train)\n",
    "knn_predictions = knn_classifier.predict(X_test)\n",
    "print(\"KNN =\\n\",knn_predictions)\n",
    "\n",
    "t_end_knn = time.time()\n",
    "\n",
    "t_begin_svm = time.time()\n",
    "# SVM\n",
    "svm_classifier =  svm.SVC(kernel='linear',probability=True)\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "svm_predictions = svm_classifier.predict(X_test)\n",
    "print(\"SVM =\\n\",svm_predictions) \n",
    "\n",
    "t_end_svm = time.time()\n",
    "\n",
    "t_knn = t_end_knn - t_begin_knn \n",
    "t_svm = t_end_svm - t_begin_svm \n",
    "print(\"temps Knn : \",t_knn,\"\\ntemps Svm : \",t_svm)\n",
    "#SVM est plus long que KNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a459392a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kNN (n_neighbors=3) :\n",
      "Exactitude : 0.895\n",
      "Rapport de classification :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      1.00      0.95        19\n",
      "           1       0.89      0.96      0.93        26\n",
      "           2       0.88      0.83      0.86        18\n",
      "           3       0.88      0.82      0.85        17\n",
      "           4       0.86      0.89      0.88        28\n",
      "           5       1.00      0.78      0.88        18\n",
      "           6       0.80      1.00      0.89        12\n",
      "           7       0.93      0.89      0.91        28\n",
      "           8       1.00      0.92      0.96        12\n",
      "           9       0.86      0.86      0.86        22\n",
      "\n",
      "    accuracy                           0.90       200\n",
      "   macro avg       0.90      0.90      0.90       200\n",
      "weighted avg       0.90      0.90      0.89       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Évaluez les performances de kNN\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=3)\n",
    "knn_classifier.fit(X_train, y_train)\n",
    "knn_predictions = knn_classifier.predict(X_test)\n",
    "print(\"kNN (n_neighbors=3) :\")\n",
    "print(\"Exactitude :\", accuracy_score(y_test, knn_predictions))  #nombredepredictioncorrect/echantillons\n",
    "print(\"Rapport de classification :\")\n",
    "print(classification_report(y_test, knn_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f1a89228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM :\n",
      "Exactitude : 0.905\n",
      "Rapport de classification :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        19\n",
      "           1       0.89      0.96      0.93        26\n",
      "           2       0.83      0.83      0.83        18\n",
      "           3       1.00      0.82      0.90        17\n",
      "           4       0.84      0.93      0.88        28\n",
      "           5       1.00      0.89      0.94        18\n",
      "           6       0.85      0.92      0.88        12\n",
      "           7       0.90      0.96      0.93        28\n",
      "           8       1.00      0.92      0.96        12\n",
      "           9       0.85      0.77      0.81        22\n",
      "\n",
      "    accuracy                           0.91       200\n",
      "   macro avg       0.92      0.90      0.91       200\n",
      "weighted avg       0.91      0.91      0.90       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Évaluez les performances de SVM\n",
    "print(\"SVM :\")\n",
    "print(\"Exactitude :\", accuracy_score(y_test, svm_predictions))\n",
    "print(\"Rapport de classification :\")\n",
    "print(classification_report(y_test, svm_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "91dd3a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sortie de predict_proba() pour kNN (n_neighbors=1) :\n",
      " [[0.         0.         0.         ... 0.33333333 0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.33333333]\n",
      " [0.         0.         1.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 1.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "proba_predictions_knn = knn_classifier.predict_proba(X_test)\n",
    "print(\"Sortie de predict_proba() pour kNN (n_neighbors=1) :\\n\", proba_predictions_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67dd67b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Différences entre KNN et SVM :\n",
    "\n",
    "1)Type de modèle : KNN est un modèle basé sur l'instance, ce qui signifie qu'il mémorise\n",
    "les exemples d'entraînement et effectue des prédictions en fonction de la proximité avec ces exemples.\n",
    "En revanche, SVM est un modèle basé sur la marge qui cherche à trouver un hyperplan qui sépare les classes.\n",
    "\n",
    "2)Complexité du modèle : KNN n'apprend pas révéler un modèle, il se contente de mémoriser les données d'entraînement. \n",
    "SVM, en revanche, apprend un modèle basé sur un hyperplan qui optimise la séparation des classes.\n",
    "\n",
    "3)Sensibilité aux dimensions : KNN peut être sensible à la \"malédiction de la dimension\", ce qui signifie que sa performance peut se détériorer \n",
    "lorsque le nombre de dimensions (caractéristiques) est élevé. SVM est généralement plus robuste dans des espaces \n",
    "de grande dimension.\n",
    "\n",
    "4)Temps de calcul : KNN peut être prêté lors de la prédiction, car il doit calculer la distance entre le nouvel échantillon\n",
    "et tous les exemples d'entraînement. SVM est généralement plus rapide pour la prédiction.\n",
    "\n",
    "5)Paramètres : KNN a un paramètre important, k (le nombre de voisins à considérer), qui doit être choisi. \n",
    "SVM et également des paramètres, mais le choix du noyau (par exemple, linéaire, RBF) est souvent crucial\n",
    "\n",
    "6)Rapidité : KNN est nettement plus rapide que SVM \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24fbb83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TP03\n",
    "# Création d'un classificateur de perceptron multicouche (MLP) avec les paramètres suivants \n",
    "from sklearn . neural_network import MLPClassifier\n",
    "classifier = MLPClassifier ( activation ='relu', hidden_layer_sizes\n",
    "=(50) , solver ='sgd', max_iter =100 , learning_rate ='constant' ,\n",
    "learning_rate_init =.1 , verbose =10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "296dd519",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.17418065\n",
      "Iteration 2, loss = 1.44858655\n",
      "Iteration 3, loss = 0.83953886\n",
      "Iteration 4, loss = 0.53881144\n",
      "Iteration 5, loss = 0.41326267\n",
      "Iteration 6, loss = 0.33432925\n",
      "Iteration 7, loss = 0.27584152\n",
      "Iteration 8, loss = 0.22853011\n",
      "Iteration 9, loss = 0.19978976\n",
      "Iteration 10, loss = 0.16954100\n",
      "Iteration 11, loss = 0.14073468\n",
      "Iteration 12, loss = 0.12141298\n",
      "Iteration 13, loss = 0.10351202\n",
      "Iteration 14, loss = 0.09071210\n",
      "Iteration 15, loss = 0.07745887\n",
      "Iteration 16, loss = 0.06689012\n",
      "Iteration 17, loss = 0.05921618\n",
      "Iteration 18, loss = 0.05189489\n",
      "Iteration 19, loss = 0.04664386\n",
      "Iteration 20, loss = 0.04163526\n",
      "Iteration 21, loss = 0.03793903\n",
      "Iteration 22, loss = 0.03454676\n",
      "Iteration 23, loss = 0.03223157\n",
      "Iteration 24, loss = 0.02925507\n",
      "Iteration 25, loss = 0.02695721\n",
      "Iteration 26, loss = 0.02473040\n",
      "Iteration 27, loss = 0.02331763\n",
      "Iteration 28, loss = 0.02185135\n",
      "Iteration 29, loss = 0.02035365\n",
      "Iteration 30, loss = 0.01940905\n",
      "Iteration 31, loss = 0.01835833\n",
      "Iteration 32, loss = 0.01714660\n",
      "Iteration 33, loss = 0.01633191\n",
      "Iteration 34, loss = 0.01552994\n",
      "Iteration 35, loss = 0.01520549\n",
      "Iteration 36, loss = 0.01411145\n",
      "Iteration 37, loss = 0.01360492\n",
      "Iteration 38, loss = 0.01299715\n",
      "Iteration 39, loss = 0.01245966\n",
      "Iteration 40, loss = 0.01202676\n",
      "Iteration 41, loss = 0.01151277\n",
      "Iteration 42, loss = 0.01122225\n",
      "Iteration 43, loss = 0.01072033\n",
      "Iteration 44, loss = 0.01039881\n",
      "Iteration 45, loss = 0.01002196\n",
      "Iteration 46, loss = 0.00968048\n",
      "Iteration 47, loss = 0.00935257\n",
      "Iteration 48, loss = 0.00909761\n",
      "Iteration 49, loss = 0.00880086\n",
      "Iteration 50, loss = 0.00857532\n",
      "Iteration 51, loss = 0.00831367\n",
      "Iteration 52, loss = 0.00807105\n",
      "Iteration 53, loss = 0.00788803\n",
      "Iteration 54, loss = 0.00768389\n",
      "Iteration 55, loss = 0.00744669\n",
      "Iteration 56, loss = 0.00724605\n",
      "Iteration 57, loss = 0.00708477\n",
      "Iteration 58, loss = 0.00690531\n",
      "Iteration 59, loss = 0.00672996\n",
      "Iteration 60, loss = 0.00660007\n",
      "Iteration 61, loss = 0.00639545\n",
      "Iteration 62, loss = 0.00627812\n",
      "Iteration 63, loss = 0.00612273\n",
      "Iteration 64, loss = 0.00601692\n",
      "Iteration 65, loss = 0.00588996\n",
      "Iteration 66, loss = 0.00574863\n",
      "Iteration 67, loss = 0.00564904\n",
      "Iteration 68, loss = 0.00552065\n",
      "Iteration 69, loss = 0.00541702\n",
      "Iteration 70, loss = 0.00529877\n",
      "Iteration 71, loss = 0.00520330\n",
      "Iteration 72, loss = 0.00510896\n",
      "Iteration 73, loss = 0.00500944\n",
      "Iteration 74, loss = 0.00492099\n",
      "Iteration 75, loss = 0.00484164\n",
      "Iteration 76, loss = 0.00477470\n",
      "Iteration 77, loss = 0.00466956\n",
      "Iteration 78, loss = 0.00459080\n",
      "Iteration 79, loss = 0.00452014\n",
      "Iteration 80, loss = 0.00444539\n",
      "Iteration 81, loss = 0.00435548\n",
      "Iteration 82, loss = 0.00429863\n",
      "Iteration 83, loss = 0.00422456\n",
      "Iteration 84, loss = 0.00416476\n",
      "Iteration 85, loss = 0.00408886\n",
      "Iteration 86, loss = 0.00403514\n",
      "Iteration 87, loss = 0.00397207\n",
      "Iteration 88, loss = 0.00390987\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Prediction = \n",
      " ['5' '8' '9' '5' '9' '2' '1' '9' '6' '9' '9' '9' '8' '7' '1' '3' '0' '3'\n",
      " '6' '9' '2' '6' '1' '3' '8' '3' '4' '2' '3' '1' '6' '2' '4' '2' '3' '2'\n",
      " '5' '5' '0' '4' '6' '7' '7' '2' '7' '7' '4' '8' '0' '8' '0' '4' '1' '4'\n",
      " '7' '2' '7' '7' '9' '0' '6' '9' '7' '8' '1' '0' '7' '9' '3' '4' '9' '0'\n",
      " '5' '7' '0' '7' '2' '6' '7' '3' '6' '8' '9' '5' '0' '3' '8' '8' '1' '8'\n",
      " '9' '4' '3' '0' '4' '1' '4' '4' '7' '8' '3' '6' '4' '7' '2' '8' '2' '9'\n",
      " '8' '5' '5' '9' '7' '0' '4' '6' '1' '1' '7' '8' '9' '1' '9' '4' '9' '2'\n",
      " '2' '1' '6' '1' '9' '5' '4' '5' '8' '7' '5' '8' '0' '8' '6' '8' '7' '0'\n",
      " '5' '5' '7' '7' '2' '0' '4' '2' '7' '4' '0' '2' '8' '2' '6' '6' '6' '6'\n",
      " '9' '2' '1' '8' '4' '4' '5' '4' '7' '1' '0' '7' '6' '2' '9' '4' '7' '3'\n",
      " '1' '4' '5' '9' '1' '1' '5' '0' '2' '7' '7' '9' '3' '5' '5' '0' '4' '8'\n",
      " '9' '2']\n",
      "exactitude : 84.00% \n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y , test_size=0.2)\n",
    "#TP03\n",
    "# Création d'un classificateur de perceptron monocouche avec les paramètres suivants \n",
    "from sklearn . neural_network import MLPClassifier\n",
    "classifier = MLPClassifier ( activation ='relu', hidden_layer_sizes\n",
    "=(20) , solver ='sgd', max_iter =100 , learning_rate ='constant' ,\n",
    "learning_rate_init =.1 , verbose =10)\n",
    "\n",
    "# Entraînement du réseau de neurones en utilisant des données d'entraînement X_train et les étiquettes y_train\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Effectue des prédictions sur les données de test X_test\n",
    "prediction = classifier.predict(X_test)\n",
    "print(\"Prediction = \\n\",prediction)\n",
    "\n",
    "# Affiche les prédictions\n",
    "accuracy=accuracy_score(y_test,prediction)\n",
    "\n",
    "# Calcul de l'exactitude en comparant les prédictions aux étiquettes réelles (y_test)f\n",
    "print(\"exactitude : {:.2f}% \".format(accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "14e9e84b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.35714020\n",
      "Iteration 2, loss = 2.35201252\n",
      "Iteration 3, loss = 2.34481940\n",
      "Iteration 4, loss = 2.33607252\n",
      "Iteration 5, loss = 2.32570190\n",
      "Iteration 6, loss = 2.31427777\n",
      "Iteration 7, loss = 2.30254480\n",
      "Iteration 8, loss = 2.29119900\n",
      "Iteration 9, loss = 2.28006062\n",
      "Iteration 10, loss = 2.26961362\n",
      "Iteration 11, loss = 2.25976513\n",
      "Iteration 12, loss = 2.25064479\n",
      "Iteration 13, loss = 2.24186733\n",
      "Iteration 14, loss = 2.23366302\n",
      "Iteration 15, loss = 2.22605803\n",
      "Iteration 16, loss = 2.21862659\n",
      "Iteration 17, loss = 2.21119305\n",
      "Iteration 18, loss = 2.20363731\n",
      "Iteration 19, loss = 2.19578986\n",
      "Iteration 20, loss = 2.18753463\n",
      "Iteration 21, loss = 2.17868924\n",
      "Iteration 22, loss = 2.16919274\n",
      "Iteration 23, loss = 2.15900439\n",
      "Iteration 24, loss = 2.14810999\n",
      "Iteration 25, loss = 2.13655464\n",
      "Iteration 26, loss = 2.12434781\n",
      "Iteration 27, loss = 2.11142667\n",
      "Iteration 28, loss = 2.09780252\n",
      "Iteration 29, loss = 2.08357484\n",
      "Iteration 30, loss = 2.06870618\n",
      "Iteration 31, loss = 2.05307199\n",
      "Iteration 32, loss = 2.03645254\n",
      "Iteration 33, loss = 2.01878164\n",
      "Iteration 34, loss = 2.00001799\n",
      "Iteration 35, loss = 1.98020514\n",
      "Iteration 36, loss = 1.95942255\n",
      "Iteration 37, loss = 1.93772535\n",
      "Iteration 38, loss = 1.91509127\n",
      "Iteration 39, loss = 1.89154560\n",
      "Iteration 40, loss = 1.86727323\n",
      "Iteration 41, loss = 1.84228718\n",
      "Iteration 42, loss = 1.81661683\n",
      "Iteration 43, loss = 1.79033457\n",
      "Iteration 44, loss = 1.76349908\n",
      "Iteration 45, loss = 1.73618792\n",
      "Iteration 46, loss = 1.70840600\n",
      "Iteration 47, loss = 1.68019981\n",
      "Iteration 48, loss = 1.65159583\n",
      "Iteration 49, loss = 1.62266081\n",
      "Iteration 50, loss = 1.59362246\n",
      "Iteration 51, loss = 1.56467646\n",
      "Iteration 52, loss = 1.53570813\n",
      "Iteration 53, loss = 1.50673659\n",
      "Iteration 54, loss = 1.47772142\n",
      "Iteration 55, loss = 1.44873188\n",
      "Iteration 56, loss = 1.41983727\n",
      "Iteration 57, loss = 1.39112627\n",
      "Iteration 58, loss = 1.36266927\n",
      "Iteration 59, loss = 1.33448527\n",
      "Iteration 60, loss = 1.30662980\n",
      "Iteration 61, loss = 1.27914792\n",
      "Iteration 62, loss = 1.25204701\n",
      "Iteration 63, loss = 1.22533822\n",
      "Iteration 64, loss = 1.19904578\n",
      "Iteration 65, loss = 1.17316218\n",
      "Iteration 66, loss = 1.14771805\n",
      "Iteration 67, loss = 1.12274012\n",
      "Iteration 68, loss = 1.09822677\n",
      "Iteration 69, loss = 1.07419535\n",
      "Iteration 70, loss = 1.05065261\n",
      "Iteration 71, loss = 1.02759949\n",
      "Iteration 72, loss = 1.00504852\n",
      "Iteration 73, loss = 0.98300039\n",
      "Iteration 74, loss = 0.96145325\n",
      "Iteration 75, loss = 0.94039649\n",
      "Iteration 76, loss = 0.91983412\n",
      "Iteration 77, loss = 0.89976832\n",
      "Iteration 78, loss = 0.88019376\n",
      "Iteration 79, loss = 0.86110053\n",
      "Iteration 80, loss = 0.84248800\n",
      "Iteration 81, loss = 0.82435599\n",
      "Iteration 82, loss = 0.80667424\n",
      "Iteration 83, loss = 0.78944025\n",
      "Iteration 84, loss = 0.77265364\n",
      "Iteration 85, loss = 0.75629615\n",
      "Iteration 86, loss = 0.74036400\n",
      "Iteration 87, loss = 0.72485199\n",
      "Iteration 88, loss = 0.70975374\n",
      "Iteration 89, loss = 0.69505565\n",
      "Iteration 90, loss = 0.68074243\n",
      "Iteration 91, loss = 0.66680644\n",
      "Iteration 92, loss = 0.65323536\n",
      "Iteration 93, loss = 0.64001720\n",
      "Iteration 94, loss = 0.62714268\n",
      "Iteration 95, loss = 0.61459716\n",
      "Iteration 96, loss = 0.60236593\n",
      "Iteration 97, loss = 0.59044467\n",
      "Iteration 98, loss = 0.57882562\n",
      "Iteration 99, loss = 0.56750070\n",
      "Iteration 100, loss = 0.55646339\n",
      "Prediction = \n",
      " ['4' '7' '2' '8' '3' '4' '8' '4' '7' '3' '9' '2' '7' '0' '2' '1' '6' '7'\n",
      " '0' '8' '5' '1' '8' '1' '1' '8' '6' '5' '0' '8' '6' '0' '4' '3' '0' '6'\n",
      " '1' '6' '0' '3' '7' '0' '1' '9' '6' '1' '7' '0' '4' '1' '6' '2' '8' '2'\n",
      " '7' '1' '0' '2' '1' '0' '5' '1' '1' '2' '7' '1' '3' '5' '7' '7' '8' '7'\n",
      " '1' '7' '8' '1' '7' '4' '3' '7' '0' '4' '8' '7' '8' '4' '8' '8' '8' '2'\n",
      " '8' '2' '2' '5' '6' '8' '4' '2' '9' '2' '7' '0' '1' '7' '8' '9' '4' '1'\n",
      " '6' '2' '4' '5' '1' '4' '6' '2' '3' '4' '8' '8' '4' '0' '0' '9' '2' '2'\n",
      " '6' '2' '4' '1' '2' '3' '1' '1' '8' '6' '3' '8' '6' '5' '5' '2' '1' '4'\n",
      " '7' '3' '6' '7' '0' '7' '8' '0' '8' '4' '3' '7' '5' '7' '2' '0' '2' '6'\n",
      " '1' '3' '6' '9' '6' '3' '1' '0' '8' '7' '8' '9' '0' '9' '7' '2' '3' '6'\n",
      " '7' '7' '5' '0' '2' '6' '3' '2' '5' '8' '2' '6' '0' '4' '5' '2' '5' '4'\n",
      " '8' '8' '4' '8' '2' '5' '8' '6' '9' '7' '4' '8' '6' '9' '5' '8' '0' '2'\n",
      " '0' '0' '7' '3' '4' '4' '0' '6' '0' '3' '6' '7' '1' '7' '5' '9' '0' '2'\n",
      " '5' '0' '1' '4' '1' '0' '2' '5' '8' '0' '2' '3' '2' '8' '7' '7' '7' '6'\n",
      " '2' '3' '7' '2' '0' '6' '6' '6' '6' '7' '1' '3' '1' '1' '1' '6' '4' '4'\n",
      " '0' '7' '7' '7' '7' '7' '9' '3' '0' '4' '3' '4' '9' '0' '7' '6' '4' '8'\n",
      " '7' '2' '5' '1' '3' '2' '2' '5' '6' '0' '8' '3' '4' '2' '7' '9' '0' '1'\n",
      " '8' '1' '5' '2' '7' '1' '7' '0' '4' '7' '7' '9' '0' '6' '0' '7' '1' '0'\n",
      " '7' '2' '4' '3' '2' '3' '7' '9' '5' '0' '1' '8' '0' '3' '8' '4' '6' '8'\n",
      " '1' '0' '8' '3' '3' '4' '1' '6' '1' '3' '2' '8' '7' '8' '5' '0' '7' '1'\n",
      " '1' '7' '7' '2' '9' '6' '8' '8' '0' '7' '8' '1' '3' '3' '4' '3' '1' '2'\n",
      " '7' '4' '8' '9' '7' '7' '4' '4' '1' '1' '1' '7' '3' '0' '1' '3' '2' '4'\n",
      " '3' '1' '5' '7' '8' '7' '4' '8' '2' '6' '3' '3' '0' '1' '3' '1' '6' '8'\n",
      " '3' '1' '6' '6' '0' '4' '2' '3' '7' '7' '7' '1' '1' '7' '3' '0' '4' '2'\n",
      " '7' '6' '0' '5' '1' '6' '4' '7' '6' '4' '7' '9' '6' '2' '7' '3' '3' '2'\n",
      " '5' '3' '2' '9' '1' '2' '4' '3' '6' '7' '1' '6' '4' '0' '6' '9' '0' '5'\n",
      " '5' '8' '0' '6' '2' '7' '9' '4' '6' '5' '7' '0' '7' '9' '6' '4' '5' '4'\n",
      " '8' '2' '5' '4' '9' '8' '4' '6' '4' '2' '6' '7' '4' '0' '4' '6' '9' '3'\n",
      " '7' '6' '4' '7' '6' '7' '7' '0' '4' '6' '1' '5' '0' '6' '0' '3' '7' '2'\n",
      " '5' '2' '0' '7' '8' '3' '8' '7' '6' '7' '3' '8' '1' '0' '1' '0' '6' '1'\n",
      " '8' '8' '5' '7' '7' '3' '0' '7' '7' '2' '0' '3' '9' '0' '7' '4' '0' '5'\n",
      " '8' '2' '3' '2' '7' '4' '4' '6' '1' '5' '1' '8' '2' '5' '6' '5' '6' '2'\n",
      " '7' '0' '7' '0' '6' '6' '8' '4' '7' '0' '4' '4' '7' '9' '3' '4' '0' '6'\n",
      " '3' '2' '6' '9' '6' '7' '7' '1' '7' '5' '5' '7' '1' '2' '6' '2' '7' '1'\n",
      " '0' '1' '1' '1' '4' '3' '4' '4' '6' '6' '9' '3' '4' '7' '4' '4' '8' '3'\n",
      " '3' '7' '2' '4' '1' '1' '3' '7' '4' '8' '5' '0' '5' '8' '8' '4' '3' '7'\n",
      " '1' '5' '2' '8' '4' '6' '8' '2' '7' '0' '2' '8' '1' '3' '6' '3' '9' '7'\n",
      " '8' '5' '7' '8' '7' '4' '0' '7' '2' '1' '3' '7' '1' '8' '6' '5' '4' '3'\n",
      " '5' '4' '3' '1' '0' '0' '7' '1' '1' '8' '7' '1' '0' '6' '1' '5' '0' '7'\n",
      " '0' '8' '9' '7' '8' '3' '6' '1' '0' '5' '7' '0' '1' '6' '7' '8' '0' '2'\n",
      " '3' '0' '0' '0' '8' '5' '5' '3' '3' '1' '6' '6' '4' '7' '2' '3' '3' '6'\n",
      " '7' '5' '7' '0' '1' '7' '4' '1' '8' '8' '6' '5' '9' '8' '0' '1' '8' '8'\n",
      " '1' '5' '3' '1' '7' '3' '1' '0' '8' '2' '3' '4' '7' '5' '6' '1' '7' '6'\n",
      " '3' '4' '6' '0' '4' '6' '6' '5' '8' '1' '0' '5' '7' '6' '1' '8' '8' '6'\n",
      " '8' '7' '7' '8' '1' '0' '5' '1']\n",
      "exactitude : 74.62% \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#essai sur les données X_hog\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_hog,y , test_size=0.8)\n",
    "classifier = MLPClassifier ( activation ='relu', hidden_layer_sizes\n",
    "=(20) , solver ='sgd', max_iter =100 , learning_rate ='constant' ,\n",
    "learning_rate_init =.1 , verbose =10)\n",
    "\n",
    "# Entraînement du réseau de neurones en utilisant des données d'entraînement X_train et les étiquettes y_train\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Effectue des prédictions sur les données de test X_test\n",
    "prediction = classifier.predict(X_test)\n",
    "print(\"Prediction = \\n\",prediction)\n",
    "\n",
    "# Affiche les prédictions\n",
    "accuracy=accuracy_score(y_test,prediction)\n",
    "\n",
    "# Calcul de l'exactitude en comparant les prédictions aux étiquettes réelles (y_test)f\n",
    "print(\"exactitude : {:.2f}% \".format(accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "da81657f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.34761854\n",
      "Iteration 2, loss = 2.33462689\n",
      "Iteration 3, loss = 2.31875124\n",
      "Iteration 4, loss = 2.30456564\n",
      "Iteration 5, loss = 2.29160946\n",
      "Iteration 6, loss = 2.27702180\n",
      "Iteration 7, loss = 2.25917346\n",
      "Iteration 8, loss = 2.23910946\n",
      "Iteration 9, loss = 2.21284099\n",
      "Iteration 10, loss = 2.18288793\n",
      "Iteration 11, loss = 2.14720121\n",
      "Iteration 12, loss = 2.10471456\n",
      "Iteration 13, loss = 2.05446607\n",
      "Iteration 14, loss = 1.99643574\n",
      "Iteration 15, loss = 1.92817206\n",
      "Iteration 16, loss = 1.84851403\n",
      "Iteration 17, loss = 1.76020108\n",
      "Iteration 18, loss = 1.66372409\n",
      "Iteration 19, loss = 1.55972594\n",
      "Iteration 20, loss = 1.45060213\n",
      "Iteration 21, loss = 1.33273379\n",
      "Iteration 22, loss = 1.22496105\n",
      "Iteration 23, loss = 1.12443144\n",
      "Iteration 24, loss = 1.03349461\n",
      "Iteration 25, loss = 0.95388815\n",
      "Iteration 26, loss = 0.88628287\n",
      "Iteration 27, loss = 0.82165272\n",
      "Iteration 28, loss = 0.76345534\n",
      "Iteration 29, loss = 0.71379852\n",
      "Iteration 30, loss = 0.67022663\n",
      "Iteration 31, loss = 0.62927867\n",
      "Iteration 32, loss = 0.58791927\n",
      "Iteration 33, loss = 0.55964209\n",
      "Iteration 34, loss = 0.52445363\n",
      "Iteration 35, loss = 0.49363573\n",
      "Iteration 36, loss = 0.47093369\n",
      "Iteration 37, loss = 0.43973046\n",
      "Iteration 38, loss = 0.42434937\n",
      "Iteration 39, loss = 0.39363104\n",
      "Iteration 40, loss = 0.37336489\n",
      "Iteration 41, loss = 0.35787622\n",
      "Iteration 42, loss = 0.33158912\n",
      "Iteration 43, loss = 0.31536579\n",
      "Iteration 44, loss = 0.30377516\n",
      "Iteration 45, loss = 0.29031154\n",
      "Iteration 46, loss = 0.27731379\n",
      "Iteration 47, loss = 0.26298642\n",
      "Iteration 48, loss = 0.24848466\n",
      "Iteration 49, loss = 0.23975493\n",
      "Iteration 50, loss = 0.23200655\n",
      "Iteration 51, loss = 0.21888168\n",
      "Iteration 52, loss = 0.20504236\n",
      "Iteration 53, loss = 0.19698963\n",
      "Iteration 54, loss = 0.18431052\n",
      "Iteration 55, loss = 0.17528509\n",
      "Iteration 56, loss = 0.16891462\n",
      "Iteration 57, loss = 0.16234413\n",
      "Iteration 58, loss = 0.15327776\n",
      "Iteration 59, loss = 0.14726342\n",
      "Iteration 60, loss = 0.13905284\n",
      "Iteration 61, loss = 0.13140244\n",
      "Iteration 62, loss = 0.12236667\n",
      "Iteration 63, loss = 0.12056794\n",
      "Iteration 64, loss = 0.11223010\n",
      "Iteration 65, loss = 0.10429721\n",
      "Iteration 66, loss = 0.10050054\n",
      "Iteration 67, loss = 0.09593293\n",
      "Iteration 68, loss = 0.09290640\n",
      "Iteration 69, loss = 0.08527285\n",
      "Iteration 70, loss = 0.08144492\n",
      "Iteration 71, loss = 0.07910380\n",
      "Iteration 72, loss = 0.07324778\n",
      "Iteration 73, loss = 0.06876134\n",
      "Iteration 74, loss = 0.06665726\n",
      "Iteration 75, loss = 0.06235113\n",
      "Iteration 76, loss = 0.05951871\n",
      "Iteration 77, loss = 0.05852651\n",
      "Iteration 78, loss = 0.05484217\n",
      "Iteration 79, loss = 0.05140985\n",
      "Iteration 80, loss = 0.04907685\n",
      "Iteration 81, loss = 0.04724515\n",
      "Iteration 82, loss = 0.04679339\n",
      "Iteration 83, loss = 0.04315529\n",
      "Iteration 84, loss = 0.04172704\n",
      "Iteration 85, loss = 0.03910472\n",
      "Iteration 86, loss = 0.03785933\n",
      "Iteration 87, loss = 0.03617606\n",
      "Iteration 88, loss = 0.03481005\n",
      "Iteration 89, loss = 0.03313774\n",
      "Iteration 90, loss = 0.03210999\n",
      "Iteration 91, loss = 0.03093939\n",
      "Iteration 92, loss = 0.02988278\n",
      "Iteration 93, loss = 0.02879036\n",
      "Iteration 94, loss = 0.02748178\n",
      "Iteration 95, loss = 0.02648172\n",
      "Iteration 96, loss = 0.02541695\n",
      "Iteration 97, loss = 0.02458198\n",
      "Iteration 98, loss = 0.02386941\n",
      "Iteration 99, loss = 0.02297541\n",
      "Iteration 100, loss = 0.02202057\n",
      "Prediction = \n",
      " ['9' '3' '9' '1' '0' '6' '3' '4' '7' '9' '4' '7' '3' '7' '7' '3' '6' '0'\n",
      " '0' '1' '7' '9' '4' '4' '9' '4' '7' '8' '2' '0' '3' '6' '3' '9' '4' '8'\n",
      " '0' '4' '8' '1' '3' '6' '0' '2' '4' '8' '6' '8' '7' '7' '1' '0' '8' '9'\n",
      " '4' '1' '1' '6' '5' '1' '2' '9' '1' '8' '8' '6' '8' '8' '5' '2' '0' '7'\n",
      " '6' '5' '8' '5' '2' '2' '3' '8' '9' '2' '6' '4' '4' '3' '8' '7' '9' '4'\n",
      " '0' '0' '7' '5' '5' '1' '9' '3' '4' '6' '9' '0' '1' '3' '8' '0' '5' '3'\n",
      " '5' '9' '2' '5' '7' '9' '4' '6' '1' '3' '1' '2' '8' '6' '2' '4' '0' '2'\n",
      " '3' '2' '3' '7' '2' '4' '5' '6' '5' '0' '8' '7' '1' '0' '1' '8' '0' '2'\n",
      " '1' '6' '7' '2' '4' '5' '9' '2' '1' '3' '1' '6' '0' '8' '0' '0' '1' '4'\n",
      " '1' '7' '0' '3' '6' '9' '7' '8' '7' '7' '7' '5' '1' '7' '1' '3' '4' '3'\n",
      " '2' '9' '5' '6' '5' '7' '2' '7' '6' '1' '6' '5' '1' '6' '7' '2' '4' '6'\n",
      " '6' '2']\n",
      "exactitude : 86.00% \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#3\n",
    "#3.1\n",
    "# Création d'un classificateur de perceptron multicouche (MLP) avec les paramètres suivants \n",
    "L=[20,30,30,20]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y , test_size=0.2)\n",
    "\n",
    "from sklearn . neural_network import MLPClassifier\n",
    "classifier = MLPClassifier ( activation ='relu', hidden_layer_sizes\n",
    "=L , solver ='sgd', max_iter =100 , learning_rate ='constant' ,\n",
    "learning_rate_init =0.01 , verbose =10)\n",
    "\n",
    "# Entraînement du réseau de neurones en utilisant des données d'entraînement X_train et les étiquettes y_train\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Effectue des prédictions sur les données de test X_test\n",
    "prediction = classifier.predict(X_test)\n",
    "print(\"Prediction = \\n\",prediction)\n",
    "\n",
    "# Affiche les prédictions\n",
    "accuracy=accuracy_score(y_test,prediction)\n",
    "\n",
    "# Calcul de l'exactitude en comparant les prédictions aux étiquettes réelles (y_test)f\n",
    "print(\"exactitude : {:.2f}% \".format(accuracy*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "00135af6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.39045496\n",
      "Iteration 2, loss = 2.38777426\n",
      "Iteration 3, loss = 2.38408577\n",
      "Iteration 4, loss = 2.37961752\n",
      "Iteration 5, loss = 2.37458310\n",
      "Iteration 6, loss = 2.36916402\n",
      "Iteration 7, loss = 2.36352705\n",
      "Iteration 8, loss = 2.35784730\n",
      "Iteration 9, loss = 2.35236893\n",
      "Iteration 10, loss = 2.34711241\n",
      "Iteration 11, loss = 2.34212397\n",
      "Iteration 12, loss = 2.33740470\n",
      "Iteration 13, loss = 2.33298223\n",
      "Iteration 14, loss = 2.32887408\n",
      "Iteration 15, loss = 2.32508719\n",
      "Iteration 16, loss = 2.32161839\n",
      "Iteration 17, loss = 2.31845870\n",
      "Iteration 18, loss = 2.31559441\n",
      "Iteration 19, loss = 2.31300855\n",
      "Iteration 20, loss = 2.31068189\n",
      "Iteration 21, loss = 2.30859402\n",
      "Iteration 22, loss = 2.30672473\n",
      "Iteration 23, loss = 2.30505391\n",
      "Iteration 24, loss = 2.30356179\n",
      "Iteration 25, loss = 2.30222971\n",
      "Iteration 26, loss = 2.30104016\n",
      "Iteration 27, loss = 2.29997645\n",
      "Iteration 28, loss = 2.29902444\n",
      "Iteration 29, loss = 2.29817076\n",
      "Iteration 30, loss = 2.29740368\n",
      "Iteration 31, loss = 2.29671198\n",
      "Iteration 32, loss = 2.29608577\n",
      "Iteration 33, loss = 2.29551657\n",
      "Iteration 34, loss = 2.29499746\n",
      "Iteration 35, loss = 2.29452184\n",
      "Iteration 36, loss = 2.29408476\n",
      "Iteration 37, loss = 2.29368109\n",
      "Iteration 38, loss = 2.29330546\n",
      "Iteration 39, loss = 2.29295521\n",
      "Iteration 40, loss = 2.29262249\n",
      "Iteration 41, loss = 2.29230724\n",
      "Iteration 42, loss = 2.29200912\n",
      "Iteration 43, loss = 2.29172147\n",
      "Iteration 44, loss = 2.29144950\n",
      "Iteration 45, loss = 2.29117388\n",
      "Iteration 46, loss = 2.29089551\n",
      "Iteration 47, loss = 2.29061784\n",
      "Iteration 48, loss = 2.29032328\n",
      "Iteration 49, loss = 2.29004924\n",
      "Iteration 50, loss = 2.28980672\n",
      "Iteration 51, loss = 2.28958744\n",
      "Iteration 52, loss = 2.28938104\n",
      "Iteration 53, loss = 2.28918402\n",
      "Iteration 54, loss = 2.28900582\n",
      "Iteration 55, loss = 2.28883376\n",
      "Iteration 56, loss = 2.28866766\n",
      "Iteration 57, loss = 2.28850643\n",
      "Iteration 58, loss = 2.28834926\n",
      "Iteration 59, loss = 2.28819048\n",
      "Iteration 60, loss = 2.28802821\n",
      "Iteration 61, loss = 2.28787013\n",
      "Iteration 62, loss = 2.28771331\n",
      "Iteration 63, loss = 2.28755227\n",
      "Iteration 64, loss = 2.28739072\n",
      "Iteration 65, loss = 2.28723157\n",
      "Iteration 66, loss = 2.28707147\n",
      "Iteration 67, loss = 2.28691342\n",
      "Iteration 68, loss = 2.28675013\n",
      "Iteration 69, loss = 2.28658699\n",
      "Iteration 70, loss = 2.28642715\n",
      "Iteration 71, loss = 2.28626779\n",
      "Iteration 72, loss = 2.28610242\n",
      "Iteration 73, loss = 2.28592958\n",
      "Iteration 74, loss = 2.28574754\n",
      "Iteration 75, loss = 2.28556202\n",
      "Iteration 76, loss = 2.28536697\n",
      "Iteration 77, loss = 2.28517659\n",
      "Iteration 78, loss = 2.28500535\n",
      "Iteration 79, loss = 2.28484204\n",
      "Iteration 80, loss = 2.28468273\n",
      "Iteration 81, loss = 2.28452299\n",
      "Iteration 82, loss = 2.28436680\n",
      "Iteration 83, loss = 2.28421738\n",
      "Iteration 84, loss = 2.28407180\n",
      "Iteration 85, loss = 2.28392876\n",
      "Iteration 86, loss = 2.28379164\n",
      "Iteration 87, loss = 2.28365842\n",
      "Iteration 88, loss = 2.28353755\n",
      "Iteration 89, loss = 2.28341933\n",
      "Iteration 90, loss = 2.28330211\n",
      "Iteration 91, loss = 2.28318775\n",
      "Iteration 92, loss = 2.28307390\n",
      "Iteration 93, loss = 2.28295993\n",
      "Iteration 94, loss = 2.28284690\n",
      "Iteration 95, loss = 2.28273333\n",
      "Iteration 96, loss = 2.28261966\n",
      "Iteration 97, loss = 2.28250587\n",
      "Iteration 98, loss = 2.28239195\n",
      "Iteration 99, loss = 2.28227721\n",
      "Iteration 100, loss = 2.28216149\n",
      "Prediction = \n",
      " ['7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7'\n",
      " '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7'\n",
      " '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7'\n",
      " '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7'\n",
      " '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7'\n",
      " '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7'\n",
      " '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7'\n",
      " '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7'\n",
      " '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7'\n",
      " '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7'\n",
      " '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7'\n",
      " '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7'\n",
      " '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7'\n",
      " '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7'\n",
      " '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7'\n",
      " '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7'\n",
      " '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7'\n",
      " '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7'\n",
      " '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7'\n",
      " '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7'\n",
      " '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7'\n",
      " '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7'\n",
      " '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7'\n",
      " '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7'\n",
      " '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7'\n",
      " '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7'\n",
      " '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7'\n",
      " '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7'\n",
      " '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7'\n",
      " '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7'\n",
      " '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7'\n",
      " '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7'\n",
      " '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7'\n",
      " '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7'\n",
      " '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7'\n",
      " '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7'\n",
      " '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7'\n",
      " '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7'\n",
      " '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7'\n",
      " '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7'\n",
      " '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7'\n",
      " '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7'\n",
      " '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7'\n",
      " '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7' '7'\n",
      " '7' '7' '7' '7' '7' '7' '7' '7']\n",
      "exactitude : 11.12% \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#3.2\n",
    "#essai sur les données X_hog\n",
    "L=[2,10,30]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_hog,y , test_size=0.8)\n",
    "classifier = MLPClassifier ( activation ='relu', hidden_layer_sizes\n",
    "=L , solver ='sgd', max_iter =100 , learning_rate ='constant' ,\n",
    "learning_rate_init =0.01 , verbose =40)\n",
    "\n",
    "# Entraînement du réseau de neurones en utilisant des données d'entraînement X_train et les étiquettes y_train\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Effectue des prédictions sur les données de test X_test\n",
    "prediction = classifier.predict(X_test)\n",
    "print(\"Prediction = \\n\",prediction)\n",
    "\n",
    "# Affiche les prédictions\n",
    "accuracy=accuracy_score(y_test,prediction)\n",
    "\n",
    "# Calcul de l'exactitude en comparant les prédictions aux étiquettes réelles (y_test)f\n",
    "print(\"exactitude : {:.2f}% \".format(accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0c139da9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrice de poids de la couche 2:(784, 20)\n",
      "Matrice de poids de la couche 3:(20, 30)\n",
      "Matrice de poids de la couche 4:(30, 30)\n",
      "Matrice de poids de la couche 5:(30, 20)\n",
      "Matrice de poids de la couche 6:(20, 10)\n"
     ]
    }
   ],
   "source": [
    "matrice_poids = classifier.coefs_\n",
    "\n",
    "for i, matrice in enumerate(matrice_poids) :\n",
    "      print(f\"Matrice de poids de la couche {i+1}:{matrice.shape}\")\n",
    "\n",
    "\"\"\"Simplification de l'entrée: En utilisant la représentation HOG,\n",
    "vous transformez l'entrée (l'image) en un ensemble de caractéristiques plus simples à interpréter\n",
    "pour le réseau de neurones\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a561ed7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rassemblons les résultats\n",
    "# Libraries :\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import time\n",
    "from skimage . feature import hog\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "# Options :\n",
    "X , y = fetch_openml(name = 'mnist_784' , version = 1 , return_X_y=True )\n",
    "X = X.to_numpy()\n",
    "y = y.to_numpy()\n",
    "classif=' ' #choisissez entre 'svm' et 'knn' et 'mlp' en l'ecrivant entre les ''\n",
    "\n",
    " # Load data , create ’train ’, ’test ’:\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y , test_size=0.2)\n",
    "# Build classifier :\n",
    "if classif == 'svm':\n",
    "    classifier = svm.SVC(kernel='linear',probability=True)\n",
    "    y_est = classifier.predict( X_test)\n",
    "\n",
    "elif classif == 'kNN':\n",
    "    classifier = KNeighborsClassifier(n_neighbors=3)\n",
    "    y_est = knn_classifier.predict(X_test)\n",
    "\n",
    "# Fit to data :\n",
    "classifier . fit (X_train,y_train)\n",
    "\n",
    "# Score :\n",
    "print (\" Training set score : \", classifier . score (X_train,y_train) )\n",
    "print (\" Testing set score : \", classifier . score (X_test,y_test) )\n",
    "\n",
    "#je tiens a rappeler que toute les experiences ont été faites dans les cellules precedantes \n",
    "# tels que la precision de chaque classe avec SVM et knn (evaluation de performance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449be03f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
